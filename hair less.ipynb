{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470e8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3285537",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71c8b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_in_crop(img):\n",
    "    img = img[0:3,:,:]\n",
    "    c, h, w = img.shape\n",
    "    \n",
    "    if w/h <= 2.4:\n",
    "        crop_h = int(w/2.4)\n",
    "        crop_w = w\n",
    "        crop_loc_h = int((h - crop_h) / 2)\n",
    "        cropped_img = img[:,crop_loc_h:crop_h+crop_loc_h,:crop_w]\n",
    "    \n",
    "    else:\n",
    "        crop_h = h\n",
    "        crop_w = int(h*2.4)\n",
    "        crop_loc_w = int((w - crop_w) / 2)\n",
    "        cropped_img = img[:,:crop_h,crop_loc_w:crop_w+crop_loc_w]\n",
    "    \n",
    "  \n",
    "    resized_img = transforms.Resize((160, 384))(cropped_img)\n",
    "    return resized_img\n",
    "\n",
    "def zoom_out_crop(img):\n",
    "    img = img[0:3,:,:]\n",
    "    c, h, w = img.shape\n",
    "    aspect_ratio = w/h\n",
    "    zeros_grid = torch.zeros(3,160,384)\n",
    "\n",
    "    if w/h <= 2.4:\n",
    "        new_h = 160\n",
    "        new_w = int(aspect_ratio * new_h)\n",
    "    else:\n",
    "        new_w = 384\n",
    "        new_h = int(new_w / aspect_ratio)\n",
    "\n",
    "\n",
    "    resized_img = transforms.Resize((new_h, new_w))(img)\n",
    "    left_pad = int((384-new_w)/2)\n",
    "    right_pad = 384 - new_w - left_pad\n",
    "    top_pad = int((160-new_h)/2)\n",
    "    bottom_pad = 160 - new_h - top_pad\n",
    "    padded_image = torch.nn.functional.pad(resized_img, (left_pad, right_pad, top_pad, bottom_pad))\n",
    "    return padded_image\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0357b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hair_implants():\n",
    "    def __init__ (self, num_pictures):\n",
    "        pic_location = \"C:/Users/shmue/OneDrive/Desktop/3D_AI/NewTsukubaStereoDataset/illumination/daylight/left\"\n",
    "        y_pics = \"C:/Users/shmue/OneDrive/Desktop/3D_AI/NewTsukubastereoDataset/illumination/daylight/right\"\n",
    "        data = os.listdir(pic_location)\n",
    "        data = np.random.choice(data, num_pictures)\n",
    "        self.data = data\n",
    "        y_data = [file_name.replace('L', 'R') for file_name in self.data]\n",
    "        self.y_data = y_data\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        pic_width = 384\n",
    "        pic_height = 160        \n",
    "        self.bag_of_hair = torch.zeros((num_pictures, 3 * pic_width * pic_height), device = device, dtype = torch.float32)\n",
    "        self.y_values = torch.zeros((num_pictures, 3 * pic_width * pic_height), device = device, dtype = torch.float32)\n",
    "        for t, hair in enumerate(data):\n",
    "            img = Image.open(pic_location + \"/\" + hair)\n",
    "            converted_tensor = zoom_out_crop(convert_tensor(img))\n",
    "            self.bag_of_hair[t,:] = torch.flatten(converted_tensor)\n",
    "        for t, hair in enumerate(y_data):\n",
    "            img = Image.open(y_pics + \"/\" + hair)\n",
    "            converted_tensor = zoom_out_crop(convert_tensor(img))\n",
    "            self.y_values[t, :] = torch.flatten(converted_tensor)\n",
    "        \n",
    "        \n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5265a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_location = \"C:/Users/shmue/girl.jpg\"\n",
    "convert_tensor = transforms.ToTensor()\n",
    "img = Image.open(pic_location)\n",
    "converted_tensor = convert_tensor(img)\n",
    "if converted_tensor.shape[2]/converted_tensor.shape[1] <= 2.4:\n",
    "    crop_h = int(converted_tensor.shape[2]/2.4)\n",
    "    crop_w = converted_tensor.shape[2]\n",
    "    crop_loc_h = int((converted_tensor.shape[1] - crop_h) / 2)\n",
    "    cropped_img = converted_tensor[:,crop_loc_h:crop_h+crop_loc_h,:crop_w]\n",
    "    \n",
    "else:\n",
    "    crop_h = converted_tensor.shape[1]\n",
    "    crop_w = int(converted_tensor.shape[1]*2.4)\n",
    "    crop_loc_w = int((converted_tensor.shape[2] - crop_w) / 2)\n",
    "    cropped_img = converted_tensor[:,:crop_h,crop_loc_w:crop_w+crop_loc_w]\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70fba635",
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_location = \"C:/Users/shmue/OneDrive/Desktop/3D_AI/NewTsukubaStereoDataset/illumination/daylight/left/tsukuba_daylight_L_00001.png\"\n",
    "convert_tensor = transforms.ToTensor()\n",
    "img = Image.open(pic_location)\n",
    "converted_tensor = convert_tensor(img)\n",
    "transform = transforms.ToPILImage()\n",
    "img = transform(zoom_out_crop(converted_tensor))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71171320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ai_3d(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ai_3d, self).__init__()\n",
    "        self.group_1_conv1 = torch.nn.Conv2d(3, 64, 3, padding =1, device=device,dtype=torch.float32)\n",
    "        torch.nn.init.normal_(self.group_1_conv1.weight, mean=0.1 , std=0.05)\n",
    "        self.group_1_relu1 = torch.nn.ReLU()\n",
    "        self.group_1_pool1 = torch.nn.MaxPool2d(2, stride = 2)\n",
    "        self.group_2_conv1 = torch.nn.Conv2d(64, 128, 3, padding =1, device=device,dtype=torch.float32)\n",
    "        self.group_2_relu1 = torch.nn.ReLU()\n",
    "        self.group_2_pool1 = torch.nn.MaxPool2d(2, stride = 2)\n",
    "        self.group_3_conv1 = torch.nn.Conv2d(128, 256, 3, padding =1, device=device,dtype=torch.float32)\n",
    "        self.group_3_relu1 = torch.nn.ReLU()\n",
    "        self.group_3_conv2 = torch.nn.Conv2d(256, 256, 3, padding =1, device=device,dtype=torch.float32)\n",
    "        self.group_3_relu2 = torch.nn.ReLU()\n",
    "        self.group_3_pool1 = torch.nn.MaxPool2d(2, stride = 2)\n",
    "        self.group_4_conv1 = torch.nn.Conv2d(256, 512, 3, padding =1, device=device,dtype=torch.float32)\n",
    "        self.group_4_relu1 = torch.nn.ReLU()\n",
    "        self.group_4_conv2 = torch.nn.Conv2d(512, 512, 3, padding =1, device=device,dtype=torch.float32)\n",
    "        self.group_4_relu2 = torch.nn.ReLU()\n",
    "        self.group_4_pool1 = torch.nn.MaxPool2d(2, stride = 2)\n",
    "        self.group_5_conv1 = torch.nn.Conv2d(512, 512, 3, padding =1, device=device,dtype=torch.float32)\n",
    "        self.group_5_relu1 = torch.nn.ReLU()\n",
    "        self.group_5_conv2 = torch.nn.Conv2d(512, 512, 3, padding =1, device=device,dtype=torch.float32)\n",
    "        self.group_5_relu2 = torch.nn.ReLU()\n",
    "        self.group_5_pool1 = torch.nn.MaxPool2d(2, stride = 2)\n",
    "        self.group_6_flatten1 = torch.nn.Flatten()\n",
    "        self.group_6_fc1 = torch.nn.Linear(30720, 512, device=device,dtype=torch.float32)\n",
    "        self.group_6_relu1 = torch.nn.ReLU()\n",
    "        self.group_6_drop1 = torch.nn.Dropout()\n",
    "        self.group_7_fc1 = torch.nn.Linear(512, 512, device=device,dtype=torch.float32)\n",
    "        self.group_7_relu1 = torch.nn.ReLU()\n",
    "        self.group_7_drop1 = torch.nn.Dropout()\n",
    "        self.group_8_fc1 = torch.nn.Linear(512, 1980, device=device,dtype=torch.float32)\n",
    "        self.bn_pool4 = torch.nn.BatchNorm2d(512, device=device)\n",
    "        self.pred4 = torch.nn.Conv2d(512, 33, 3, stride = 1, padding =0, device=device,dtype=torch.float32)\n",
    "        self.bn_pool3 = torch.nn.BatchNorm2d(256, device=device)\n",
    "        self.pred3 = torch.nn.Conv2d(256, 33, 3, stride = 1, padding =0, device=device,dtype=torch.float32)\n",
    "        self.bn_pool2 = torch.nn.BatchNorm2d(128, device=device)\n",
    "        self.pred2 = torch.nn.Conv2d(128, 33, 3, stride = 1, padding =0, device=device,dtype=torch.float32)\n",
    "        self.bn_pool1 = torch.nn.BatchNorm2d(64, device=device)\n",
    "        self.pred1 = torch.nn.Conv2d(64, 33, 3, stride = 1, padding =0, device=device,dtype=torch.float32)\n",
    "        scale = 1 #1\n",
    "        self.pred_1_relu = torch.nn.ReLU()\n",
    "        self.pred_1_convt = torch.nn.ConvTranspose2d(33, 33, 1+2, stride = 1, padding =0, bias = False, device=device,dtype=torch.float32)\n",
    "        scale *= 2 #2\n",
    "        self.pred_2_relu = torch.nn.ReLU()\n",
    "        self.pred_2_convt = torch.nn.ConvTranspose2d(33, 33, 2*scale+4, stride = scale, padding =scale//2, bias = False, device=device,dtype=torch.float32)\n",
    "        scale *= 2 #4\n",
    "        self.pred_3_relu = torch.nn.ReLU()\n",
    "        self.pred_3_convt = torch.nn.ConvTranspose2d(33, 33, 2*scale+8, stride = scale, padding =scale//2, bias = False, device=device,dtype=torch.float32)\n",
    "        scale *= 2 #8\n",
    "        self.pred_4_relu = torch.nn.ReLU()\n",
    "        self.pred_4_convt = torch.nn.ConvTranspose2d(33, 33, 2*scale+16, stride = scale, padding =scale//2, bias = False, device=device,dtype=torch.float32)\n",
    "        scale *= 2 #16\n",
    "        self.pred_5_relu = torch.nn.ReLU()\n",
    "        self.pred_5_convt = torch.nn.ConvTranspose2d(33, 33, 2*scale, stride = scale, padding =scale//2, bias = False, device=device,dtype=torch.float32)\n",
    "        self.feat_act_relu = torch.nn.ReLU()\n",
    "        scale = 2\n",
    "        self.up_convt = torch.nn.ConvTranspose2d(33, 512, 2*scale, stride = scale, padding =scale//2, bias = False, device=device,dtype=torch.float32)\n",
    "        self.up_relu = torch.nn.ReLU()\n",
    "        self.up_conv = torch.nn.Conv2d(512, 33, 3, padding =1, device=device,dtype=torch.float32)\n",
    "        self.output_conv = torch.nn.Conv2d(33, 3, 3, padding =1, device=device,dtype=torch.float32)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x,dimensions):\n",
    "        group_1 = self.group_1_conv1(x.reshape(-1, dimensions[0], dimensions[1], dimensions[2]))\n",
    "        group_1 = self.group_1_relu1(group_1)\n",
    "        group_1 = self.group_1_pool1(group_1)\n",
    "        group_2 = self.group_2_conv1(group_1)\n",
    "        group_2 = self.group_2_relu1(group_2)\n",
    "        group_2 = self.group_2_pool1(group_2)\n",
    "        group_3 = self.group_3_conv1(group_2)\n",
    "        group_3 = self.group_3_relu1(group_3)\n",
    "        group_3 = self.group_3_conv2(group_3)\n",
    "        group_3 = self.group_3_relu2(group_3)\n",
    "        group_3 = self.group_3_pool1(group_3)\n",
    "        group_4 = self.group_4_conv1(group_3)\n",
    "        group_4 = self.group_4_relu1(group_4)\n",
    "        group_4 = self.group_4_conv2(group_4)\n",
    "        group_4 = self.group_4_relu2(group_4)\n",
    "        group_4 = self.group_4_pool1(group_4)\n",
    "        group_5 = self.group_5_conv1(group_4)\n",
    "        group_5 = self.group_5_relu1(group_5)\n",
    "        group_5 = self.group_5_conv2(group_5)\n",
    "        group_5 = self.group_5_relu2(group_5)\n",
    "        group_5 = self.group_5_pool1(group_5)\n",
    "        group_6 = self.group_6_flatten1(group_5)\n",
    "        group_6 = self.group_6_fc1(group_6)\n",
    "        group_6 = self.group_6_relu1(group_6)\n",
    "        group_6 = self.group_6_drop1(group_6)\n",
    "        group_7 = self.group_7_fc1(group_6)\n",
    "        group_7 = self.group_7_relu1(group_7)\n",
    "        group_7 = self.group_7_drop1(group_7)\n",
    "        group_8_fc1 = self.group_8_fc1(group_7)\n",
    "        pred5 = group_8_fc1.reshape(-1, 33, 12, 5) \n",
    "        bn_pool4 = self.bn_pool4(group_4)\n",
    "        pred4 = self.pred4(bn_pool4)\n",
    "        bn_pool3 = self.bn_pool3(group_3)\n",
    "        pred3 = self.pred3(bn_pool3)\n",
    "        bn_pool2 = self.bn_pool2(group_2)\n",
    "        pred2 = self.pred2(bn_pool2)\n",
    "        bn_pool1 = self.bn_pool1(group_1)\n",
    "        pred1 = self.pred1(bn_pool1)\n",
    "        pred1 = self.pred_1_relu(pred1)\n",
    "        pred1 = self.pred_1_convt(pred1)\n",
    "        pred2 = self.pred_2_relu(pred2)\n",
    "        pred2 = self.pred_2_convt(pred2)\n",
    "        pred3 = self.pred_3_relu(pred3)\n",
    "        pred3 = self.pred_3_convt(pred3)\n",
    "        pred4 = self.pred_4_relu(pred4)\n",
    "        pred4 = self.pred_4_convt(pred4)\n",
    "        pred5 = self.pred_5_relu(pred5)\n",
    "        pred5 = self.pred_5_convt(pred5)\n",
    "        feat = (pred1 + pred2 + pred3 + pred4 + pred5)\n",
    "        feat_act_relu = self.feat_act_relu(feat)\n",
    "        up = self.up_convt(feat_act_relu)\n",
    "        up = self.up_relu(up)\n",
    "        up = self.up_conv(up)\n",
    "        up = self.output_conv(up)\n",
    "        up = up.reshape(-1, 184320)\n",
    "        return up\n",
    "    \n",
    "    def loss(self, input, target):\n",
    "        loss_function = torch.nn.MSELoss()\n",
    "        return loss_function(input, target)\n",
    "    \n",
    "def train(upload = False, save = False):\n",
    "    model = ai_3d()\n",
    "    if upload:\n",
    "        model.load_state_dict(torch.load(\"garbage_bag_full_of_hair\"))\n",
    "    learning_rate = 1e-6\n",
    "    epochs = 40\n",
    "    steps = 1\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)\n",
    "    for i in range(epochs):\n",
    "        data = hair_implants(10)\n",
    "        for j in range(steps):\n",
    "            results = model(data.bag_of_hair, (3, 384, 160))\n",
    "            loss = model.loss(results,data.y_values)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"epoch number - \", i ,\" - \",  loss.item())\n",
    "        if save:\n",
    "            torch.save(model.state_dict(), \"garbage_bag_full_of_hair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e525434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number -  0  -  0.2096422016620636\n",
      "epoch number -  1  -  0.1735018640756607\n",
      "epoch number -  2  -  0.14777663350105286\n",
      "epoch number -  3  -  0.05041838064789772\n",
      "epoch number -  4  -  0.12596935033798218\n",
      "epoch number -  5  -  0.09528335183858871\n",
      "epoch number -  6  -  0.22639040648937225\n",
      "epoch number -  7  -  0.25825414061546326\n",
      "epoch number -  8  -  0.10115137696266174\n",
      "epoch number -  9  -  0.09405973553657532\n",
      "epoch number -  10  -  0.10176130384206772\n",
      "epoch number -  11  -  0.13842210173606873\n",
      "epoch number -  12  -  0.16192294657230377\n",
      "epoch number -  13  -  0.15163050591945648\n",
      "epoch number -  14  -  0.1628808081150055\n",
      "epoch number -  15  -  0.1846861094236374\n",
      "epoch number -  16  -  0.1274351179599762\n",
      "epoch number -  17  -  0.08591566234827042\n",
      "epoch number -  18  -  0.168351411819458\n",
      "epoch number -  19  -  0.1397794485092163\n",
      "epoch number -  20  -  0.18676677346229553\n",
      "epoch number -  21  -  0.08725447207689285\n",
      "epoch number -  22  -  0.1220405101776123\n",
      "epoch number -  23  -  0.12733304500579834\n",
      "epoch number -  24  -  0.14790838956832886\n",
      "epoch number -  25  -  0.12994137406349182\n",
      "epoch number -  26  -  0.15621785819530487\n",
      "epoch number -  27  -  0.14770707488059998\n",
      "epoch number -  28  -  0.11859130859375\n",
      "epoch number -  29  -  0.14565236866474152\n",
      "epoch number -  30  -  0.12014473974704742\n",
      "epoch number -  31  -  0.12181298434734344\n",
      "epoch number -  32  -  0.18327374756336212\n",
      "epoch number -  33  -  0.15440633893013\n",
      "epoch number -  34  -  0.18181279301643372\n",
      "epoch number -  35  -  0.1366487443447113\n",
      "epoch number -  36  -  0.17558452486991882\n",
      "epoch number -  37  -  0.19987688958644867\n",
      "epoch number -  38  -  0.07333561033010483\n",
      "epoch number -  39  -  0.22531983256340027\n"
     ]
    }
   ],
   "source": [
    "train(save = True, upload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "219b4efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0174, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 184320])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hair = hair_implants(1)\n",
    "model = ai_3d()\n",
    "base_loss = model.loss(hair.bag_of_hair,hair.y_values)\n",
    "print(base_loss)\n",
    "output = model(hair.bag_of_hair, (3, 384, 160))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17eb707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToPILImage()\n",
    "img = transform(output.reshape(3,160,384))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718faf02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
